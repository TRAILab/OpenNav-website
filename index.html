<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="OpenNav: Open-World Outdoor Navigation with Multimodal Large Language Models">
  <meta name="keywords" content="Vision-Language-Navigation, Open Vocabulary Object Detector">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OpenNav: Open-World Outdoor Navigation with Multimodal Large Language Models</title>

  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) {
      w[l] = w[l] || []; w[l].push({
        'gtm.start':
          new Date().getTime(), event: 'gtm.js'
      }); var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
          'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-TSPQB2LZ');</script>
  <!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/TRAIL_BLACK_ICON.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TSPQB2LZ" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">OpenNav: Open-World Outdoor Navigation with Multimodal Large Language Models
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.trailab.utias.utoronto.ca/mingfeng-yuan">Mingfeng Yuan</a>,</span>
              <span class="author-block">
                <a href="https://letianwang0.wixsite.com/myhome">Letian Wang</a>,</span>
              <span class="author-block">
                <a href="https://www.trailab.utias.utoronto.ca/steven-waslander">Steven Waslander</a>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">University of Toronto</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://mfyuan.github.io/OpenNav-website/"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://mfyuan.github.io/OpenNav-website/" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="https://mfyuan.github.io/OpenNav-website/"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://mfyuan.github.io/OpenNav-website/" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

    <section class="section">
    <!div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Key Features</h2>
          <div class="content has-text-justified">
            <ul>
              <li><strong>OpenNav is the first framework to enable zero-shot outdoor navigation by directly generating trajectories with an MLLM, without pre-trained skills, motion primitives, or in-context examples, allowing navigation with open-set instructions and objects.
              <li><strong>Multi-Expert System for Robust Scene Comprehension</strong> By integrating state-of-the-art MLLMs with an open-vocabulary perception system (OVPS), OpenNav improves robustness in diverse environments. A single task-agnostic prompt and a multimodal interface between the MLLM and OVPS enhance adaptability to open-set objects and language inputs.
              <li><strong>Turning Language Instructions into Robot Actions.</strong> OpenNav combines the reasoning, code generation, and function-calling abilities of MLLMs with classical planning techniques, harnessing the benefits of both human-like
                          reasoning and geometry-compliant trajectory synthesis.
              <li><strong>Evaluation in Autonomous Vehicle Datasets (AVDs)</strong>: We validate OpenNav's performance using AVDs, offering a new approach to studying embodied intelligence with rich, labeled data from real-world navigation tasks.
            </ul>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
  </section>
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="has-text-centered">
          <!-- <embed src="./static/images/JDT3D_Architecture_v6.pdf" style="width: 80%; height: 500px;" alt="JDT3D Architecture"> -->
            <img src="./static/images/Framework2.jpg" style="width: 80%;" alt="OpenNav Architecture">
        </div>
          <h2 class="subtitle" style="text-align: justify;">
         Overview of Our <strong>OpenNav.</strong> Given the posed RGB-Lidar observation of the environment and an open-set free-form language instruction, 1) we leverages task-agnostic prompts to enable zero-shot generalization and adaptability to varied instructions; 2) MLLM generates code, which interacts with OVPS, to produce open-set multimodal scene perception outputs, and 2D bird-eye-view (BEV) value map (consists of a semantic map and occupancy map) grounded in the operation environment. 3) MLLM synthesizes a human-like coarse trajectory based on instructions, scene understanding, and its reasoning capabilities. The generated BEV value map then serves as the objective function for the motion planner, which refines the trajectory to ensure geometry-compliant navigation. Please see the next Figure for detailed pipeline, inputs, and outputs of the OVPS.
        </h2>
      </div>
    </div>
  </section>

    <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="has-text-centered">
          <!-- <embed src="./static/images/JDT3D_Architecture_v6.pdf" style="width: 80%; height: 500px;" alt="JDT3D Architecture"> -->
            <img src="./static/images/Perception.jpg" style="width: 80%;" alt="OVPS Architecture">
        </div>
        <h2 class="subtitle" style="text-align: justify;">
          Overview of <span class="dnerf"> Open Vocabulary Perception System (OVPS).</span> OVPS sequentially performs detection, segmentation, and object caption generation. Combined with 3D point clouds, the system will generate 1) multimodal observations for VLN, including text prompts and image prompts for MLLMs, 2) 3D reconstructed map, as well as 2D occupancy and semantic maps for trajectory refinement.
        </h2>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="has-text-centered">
          <!-- <embed src="./static/images/JDT3D_Architecture_v6.pdf" style="width: 80%; height: 500px;" alt="JDT3D Architecture"> -->
            <img src="./static/images/Selection.png" style="width: 80%;" alt="OpenNav Demo">
        </div>
        <h2 class="subtitle" style="text-align: justify;">
          Given a free-form language instruction and sensor observations, OpenNav is capable of generating a dense sequence of instruction-following and scene-compliant robot waypoints in a zero-shot manner for open-world navigation, effectively handling open-set objects and open-set instructions without relying on in-context examples or pre-trained skills.
        </h2>
      </div>
    </div>
  </section>

    <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="has-text-centered">
          <!-- <embed src="./static/images/JDT3D_Architecture_v6.pdf" style="width: 80%; height: 500px;" alt="JDT3D Architecture"> -->
            <img src="./static/images/Language guided 3 results.jpg" style="width: 80%;" alt="OpenNav Demo">
        </div>
        <h2 class="subtitle" style="text-align: justify;">
          Selected examples demonstrating how OpenNav utilizes Value Maps to generate task-aligned and geometry-compliant navigation trajectories: <strong>Left:</strong> User-specified tasks and trajectories generated by OpenNav; <strong>Right:</strong> Value maps illustrating trajectories generated by different algorithms based on the given tasks.
        </h2>
      </div>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://github.com/TRAILab" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              Thank you to the authors of <a href="https://github.com/nerfies/nerfies.github.io/tree/main">Nerfies</a> for the
              website template.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
